{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read YOLO data & process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PATH\n",
    "path = r'D:\\NOTEBOOK\\FYP\\final_dataset_withnon'\n",
    "train_img_path = os.path.join(path, 'images', 'train')\n",
    "train_lbl_path = os.path.join(path, 'labels', 'train')\n",
    "\n",
    "valid_img_path = os.path.join(path, 'images', 'val')\n",
    "valid_lbl_path = os.path.join(path, 'labels', 'val')\n",
    "\n",
    "test_img_path = os.path.join(path, 'images', 'test')\n",
    "test_lbl_path = os.path.join(path, 'labels', 'test')\n",
    "\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# path to save processed images\n",
    "processed_path = os.path.join(path, 'processed')\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a dataset by extracting craters from images based on YOLO-format labels and saving them in class-specific directories. \n",
    "def process_dataset(img_dir, lbl_dir, output_dir):\n",
    "    for img_file in os.listdir(img_dir):\n",
    "        if not img_file.endswith('.jpg'):\n",
    "            continue # skip non-image files\n",
    "            \n",
    "        # Get the corresponding label file\n",
    "        base_name = os.path.splitext(img_file)[0]\n",
    "        lbl_file = os.path.join(lbl_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Process single image\n",
    "        img = Image.open(os.path.join(img_dir, img_file))\n",
    "        img_w, img_h = img.size\n",
    "        \n",
    "        with open(lbl_file, 'r') as f:\n",
    "            for idx, line in enumerate(f.readlines()):\n",
    "                class_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "                # Ensure image bounds\n",
    "                x1 = int((xc - w/2) * img_w)\n",
    "                y1 = int((yc - h/2) * img_h)\n",
    "                x2 = int((xc + w/2) * img_w)\n",
    "                y2 = int((yc + h/2) * img_h)\n",
    "                                \n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(img_w, x2), min(img_h, y2)\n",
    "                \n",
    "                # Crop the crater and resize it\n",
    "                crater = img.crop((x1, y1, x2, y2))\n",
    "                crater = crater.resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "                # save\n",
    "                class_dir = os.path.join(output_dir, str(int(class_id)))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                crater.save(os.path.join(class_dir, f\"{base_name}_{idx}.jpg\"))\n",
    "\n",
    "# process all dataset\n",
    "process_dataset(train_img_path, train_lbl_path, os.path.join(processed_path, 'train'))\n",
    "process_dataset(valid_img_path, valid_lbl_path, os.path.join(processed_path, 'val'))\n",
    "process_dataset(test_img_path, test_lbl_path, os.path.join(processed_path, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a PyTorch dataset for loading crater images and their corresponding labels.\n",
    "class CraterDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        for class_id in range(NUM_CLASSES):\n",
    "            class_dir = os.path.join(data_dir, str(class_id))\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                self.data.append((os.path.join(class_dir, img_file), class_id))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert('RGB') \n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1880 images belonging to 4 classes.\n",
      "Found 413 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define data enhancement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Loading data from category folder\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'train'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'val'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 490ms/step - accuracy: 0.7910 - loss: 0.8551 - val_accuracy: 0.8959 - val_loss: 0.4531\n",
      "Epoch 2/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 462ms/step - accuracy: 0.8544 - loss: 0.4736 - val_accuracy: 0.9080 - val_loss: 0.2486\n",
      "Epoch 3/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 443ms/step - accuracy: 0.8708 - loss: 0.3867 - val_accuracy: 0.9056 - val_loss: 0.2873\n",
      "Epoch 4/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 454ms/step - accuracy: 0.8712 - loss: 0.3919 - val_accuracy: 0.9080 - val_loss: 0.2623\n",
      "Epoch 5/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 464ms/step - accuracy: 0.8761 - loss: 0.3496 - val_accuracy: 0.9225 - val_loss: 0.1998\n",
      "Epoch 6/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 457ms/step - accuracy: 0.8668 - loss: 0.3739 - val_accuracy: 0.9274 - val_loss: 0.2162\n",
      "Epoch 7/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 462ms/step - accuracy: 0.8662 - loss: 0.4249 - val_accuracy: 0.9056 - val_loss: 0.2781\n",
      "Epoch 8/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 462ms/step - accuracy: 0.8724 - loss: 0.3760 - val_accuracy: 0.9128 - val_loss: 0.2859\n",
      "Epoch 9/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 446ms/step - accuracy: 0.8955 - loss: 0.3044 - val_accuracy: 0.9153 - val_loss: 0.2396\n",
      "Epoch 10/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 463ms/step - accuracy: 0.8891 - loss: 0.2883 - val_accuracy: 0.9298 - val_loss: 0.2201\n",
      "Epoch 11/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 480ms/step - accuracy: 0.9000 - loss: 0.3006 - val_accuracy: 0.9322 - val_loss: 0.2168\n",
      "Epoch 12/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 463ms/step - accuracy: 0.8916 - loss: 0.3121 - val_accuracy: 0.9298 - val_loss: 0.2202\n",
      "Epoch 13/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 459ms/step - accuracy: 0.8950 - loss: 0.3324 - val_accuracy: 0.9128 - val_loss: 0.2520\n",
      "Epoch 14/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 452ms/step - accuracy: 0.8869 - loss: 0.3014 - val_accuracy: 0.9249 - val_loss: 0.1931\n",
      "Epoch 15/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 444ms/step - accuracy: 0.9067 - loss: 0.2584 - val_accuracy: 0.9274 - val_loss: 0.2020\n",
      "Epoch 16/30\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 460ms/step - accuracy: 0.8871 - loss: 0.3016 - val_accuracy: 0.9298 - val_loss: 0.1745\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ModelCheckpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# early_stop \n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, early_stop] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Test & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 872 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'test'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical', \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8915 - loss: 0.2350\n",
      "\n",
      "Test accuracy: 0.9128\n",
      "Test loss: 0.2465\n"
     ]
    }
   ],
   "source": [
    "# load best data\n",
    "best_model = load_model('best_model.keras')\n",
    "\n",
    "# Evaluating Model Performance\n",
    "test_loss, test_acc = best_model.evaluate(test_generator)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
    "print(f'Test loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.35      0.43        31\n",
      "           1       0.96      0.99      0.97       746\n",
      "           2       0.63      0.60      0.61        65\n",
      "           3       0.48      0.37      0.42        30\n",
      "\n",
      "    accuracy                           0.91       872\n",
      "   macro avg       0.65      0.58      0.61       872\n",
      "weighted avg       0.90      0.91      0.91       872\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 11   4  10   6]\n",
      " [  0 735  11   0]\n",
      " [  7  13  39   6]\n",
      " [  2  15   2  11]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred_classes)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
