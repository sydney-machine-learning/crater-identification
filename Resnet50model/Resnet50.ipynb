{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:02:20.685634Z",
     "start_time": "2025-04-06T03:02:17.927462Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read YOLO data & process"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:02:20.836013Z",
     "start_time": "2025-04-06T03:02:20.832155Z"
    }
   },
   "source": [
    "# DATA PATH\n",
    "path = 'final_dataset_withnon'\n",
    "train_img_path = os.path.join(path, 'images', 'train')\n",
    "train_lbl_path = os.path.join(path, 'labels', 'train')\n",
    "\n",
    "valid_img_path = os.path.join(path, 'images', 'val')\n",
    "valid_lbl_path = os.path.join(path, 'labels', 'val')\n",
    "\n",
    "test_img_path = os.path.join(path, 'images', 'test')\n",
    "test_lbl_path = os.path.join(path, 'labels', 'test')\n",
    "\n",
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# path to save processed images\n",
    "processed_path = os.path.join(path, 'processed')\n",
    "os.makedirs(processed_path, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:02:26.192256Z",
     "start_time": "2025-04-06T03:02:22.656380Z"
    }
   },
   "source": [
    "# process a dataset by extracting craters from images based on YOLO-format labels and saving them in class-specific directories. \n",
    "def process_dataset(img_dir, lbl_dir, output_dir):\n",
    "    for img_file in os.listdir(img_dir):\n",
    "        if not img_file.endswith('.jpg'):\n",
    "            continue # skip non-image files\n",
    "            \n",
    "        # Get the corresponding label file\n",
    "        base_name = os.path.splitext(img_file)[0]\n",
    "        lbl_file = os.path.join(lbl_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Process single image\n",
    "        img = Image.open(os.path.join(img_dir, img_file))\n",
    "        img_w, img_h = img.size\n",
    "        \n",
    "        with open(lbl_file, 'r') as f:\n",
    "            for idx, line in enumerate(f.readlines()):\n",
    "                class_id, xc, yc, w, h = map(float, line.strip().split())\n",
    "                # Ensure image bounds\n",
    "                x1 = int((xc - w/2) * img_w)\n",
    "                y1 = int((yc - h/2) * img_h)\n",
    "                x2 = int((xc + w/2) * img_w)\n",
    "                y2 = int((yc + h/2) * img_h)\n",
    "                                \n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(img_w, x2), min(img_h, y2)\n",
    "                \n",
    "                # Crop the crater and resize it\n",
    "                crater = img.crop((x1, y1, x2, y2))\n",
    "                crater = crater.resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "                # save\n",
    "                class_dir = os.path.join(output_dir, str(int(class_id)))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                crater.save(os.path.join(class_dir, f\"{base_name}_{idx}.jpg\"))\n",
    "\n",
    "# process all dataset\n",
    "process_dataset(train_img_path, train_lbl_path, os.path.join(processed_path, 'train'))\n",
    "process_dataset(valid_img_path, valid_lbl_path, os.path.join(processed_path, 'val'))\n",
    "process_dataset(test_img_path, test_lbl_path, os.path.join(processed_path, 'test'))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:02:26.360586Z",
     "start_time": "2025-04-06T03:02:26.210303Z"
    }
   },
   "source": [
    "# Define data enhancement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Loading data from category folder\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'train'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'val'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(processed_path, 'test'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical', \n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1880 images belonging to 4 classes.\n",
      "Found 413 images belonging to 4 classes.\n",
      "Found 872 images belonging to 4 classes.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resnet 50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T03:02:29.436884Z",
     "start_time": "2025-04-06T03:02:29.432875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score as sk_f1\n",
    "\n",
    "def multi_class_f1(y_true, y_pred):\n",
    "    def f1_np(y_true_np, y_pred_np):\n",
    "        y_true_np = np.argmax(y_true_np, axis=1)\n",
    "        y_pred_np = np.argmax(y_pred_np, axis=1)\n",
    "        f1 = sk_f1(y_true_np, y_pred_np, average='weighted')\n",
    "        return np.array(f1, dtype=np.float32)\n",
    "\n",
    "    f1_score = tf.py_function(func=f1_np, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    f1_score.set_shape([])\n",
    "    return f1_score\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:31:54.671074Z",
     "start_time": "2025-04-05T10:05:05.708719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ResNet50_model = ResNet50(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "# for layer in ResNet50_model.layers[:-10]:\n",
    "#     layer.trainable = False\n",
    "# \n",
    "# for layer in ResNet50_model.layers[-10:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# defined output layer\n",
    "x = layers.GlobalAveragePooling2D()(ResNet50_model.output)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=ResNet50_model.input, outputs=x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.001), \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[multi_class_f1]\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_resnet.keras', \n",
    "    monitor='val_multi_class_f1', \n",
    "    save_best_only=True, \n",
    "    mode='max'\n",
    ")\n",
    "# early_stop = EarlyStopping(\n",
    "#     monitor='val_multi_class_f1',\n",
    "#     patience=7,\n",
    "#     restore_best_weights=True,\n",
    "#     mode='max',\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_multi_class_f1', factor=0.5, patience=5, mode='max',verbose=0)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, reduce_lr]\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 901ms/step - loss: 1.7963 - multi_class_f1: 0.7448 - val_loss: 0.6225 - val_multi_class_f1: 0.8484 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 837ms/step - loss: 1.0167 - multi_class_f1: 0.8190 - val_loss: 0.7972 - val_multi_class_f1: 0.8471 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 859ms/step - loss: 0.7347 - multi_class_f1: 0.8146 - val_loss: 11.0686 - val_multi_class_f1: 0.8481 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 1s/step - loss: 1.3203 - multi_class_f1: 0.7886 - val_loss: 2400.0945 - val_multi_class_f1: 0.8464 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m62s\u001B[0m 1s/step - loss: 1.1023 - multi_class_f1: 0.7813 - val_loss: 6.0972 - val_multi_class_f1: 0.8492 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m62s\u001B[0m 1s/step - loss: 0.8749 - multi_class_f1: 0.7725 - val_loss: 0.5815 - val_multi_class_f1: 0.8304 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m62s\u001B[0m 1s/step - loss: 0.6123 - multi_class_f1: 0.7946 - val_loss: 0.9762 - val_multi_class_f1: 0.8486 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m59s\u001B[0m 996ms/step - loss: 0.5789 - multi_class_f1: 0.7753 - val_loss: 0.8149 - val_multi_class_f1: 0.7272 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 886ms/step - loss: 0.4671 - multi_class_f1: 0.8448 - val_loss: 1.9565 - val_multi_class_f1: 0.1282 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 857ms/step - loss: 0.6224 - multi_class_f1: 0.8573 - val_loss: 2.7390 - val_multi_class_f1: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 934ms/step - loss: 0.3920 - multi_class_f1: 0.8384 - val_loss: 0.4259 - val_multi_class_f1: 0.8557 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 837ms/step - loss: 0.3629 - multi_class_f1: 0.8860 - val_loss: 2.6009 - val_multi_class_f1: 0.0508 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 817ms/step - loss: 0.4407 - multi_class_f1: 0.8632 - val_loss: 0.6997 - val_multi_class_f1: 0.8341 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 846ms/step - loss: 0.3830 - multi_class_f1: 0.8831 - val_loss: 1.5199 - val_multi_class_f1: 0.8581 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 806ms/step - loss: 0.4586 - multi_class_f1: 0.8818 - val_loss: 4.0301 - val_multi_class_f1: 0.7132 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 845ms/step - loss: 0.3055 - multi_class_f1: 0.8939 - val_loss: 0.7513 - val_multi_class_f1: 0.8506 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 825ms/step - loss: 0.3477 - multi_class_f1: 0.9059 - val_loss: 4.2020 - val_multi_class_f1: 0.0030 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 852ms/step - loss: 0.3232 - multi_class_f1: 0.8769 - val_loss: 0.3958 - val_multi_class_f1: 0.8812 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 810ms/step - loss: 0.3510 - multi_class_f1: 0.8749 - val_loss: 0.6541 - val_multi_class_f1: 0.8542 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 812ms/step - loss: 0.2820 - multi_class_f1: 0.8983 - val_loss: 0.9703 - val_multi_class_f1: 0.6100 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 856ms/step - loss: 0.3065 - multi_class_f1: 0.8877 - val_loss: 0.7467 - val_multi_class_f1: 0.7995 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 875ms/step - loss: 0.3262 - multi_class_f1: 0.8892 - val_loss: 6.4177 - val_multi_class_f1: 0.7762 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 868ms/step - loss: 0.3896 - multi_class_f1: 0.8837 - val_loss: 1.0083 - val_multi_class_f1: 0.7634 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 902ms/step - loss: 0.3057 - multi_class_f1: 0.8844 - val_loss: 0.7412 - val_multi_class_f1: 0.8789 - learning_rate: 2.5000e-04\n",
      "Epoch 25/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 935ms/step - loss: 0.2743 - multi_class_f1: 0.8967 - val_loss: 0.5256 - val_multi_class_f1: 0.8699 - learning_rate: 2.5000e-04\n",
      "Epoch 26/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 941ms/step - loss: 0.2401 - multi_class_f1: 0.9293 - val_loss: 0.6608 - val_multi_class_f1: 0.8195 - learning_rate: 2.5000e-04\n",
      "Epoch 27/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 933ms/step - loss: 0.2252 - multi_class_f1: 0.9240 - val_loss: 0.5051 - val_multi_class_f1: 0.9001 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 896ms/step - loss: 0.2514 - multi_class_f1: 0.9259 - val_loss: 0.2033 - val_multi_class_f1: 0.9348 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 886ms/step - loss: 0.2672 - multi_class_f1: 0.9054 - val_loss: 0.3847 - val_multi_class_f1: 0.8897 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001B[1m59/59\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 896ms/step - loss: 0.2420 - multi_class_f1: 0.9207 - val_loss: 0.3066 - val_multi_class_f1: 0.9054 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:32:06.281468Z",
     "start_time": "2025-04-05T10:31:54.729178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model_resnet = load_model(\n",
    "    'best_model_resnet.keras',\n",
    "    custom_objects={'multi_class_f1': multi_class_f1}\n",
    ")\n",
    "test_loss, test_f1 = best_model_resnet.evaluate(test_generator)\n",
    "print(f'\\n✅ Test Macro F1-score: {test_f1:.4f}')\n",
    "print(f'✅ Test loss: {test_loss:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 188ms/step - loss: 0.2796 - multi_class_f1: 0.9353\n",
      "\n",
      "✅ Test Macro F1-score: 0.9435\n",
      "✅ Test loss: 0.2699\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:32:15.483970Z",
     "start_time": "2025-04-05T10:32:06.311297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_probs = best_model_resnet.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_generator.classes \n",
    "\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, y_pred_classes,\n",
    "    target_names=class_names,\n",
    "    output_dict=True,\n",
    "    zero_division=1\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Per-Class F1-scores:\")\n",
    "for class_name in class_names:\n",
    "    f1 = report[class_name]['f1-score']\n",
    "    print(f\"  {class_name}: {f1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n📃 Full classification report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names, zero_division=1))\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred_classes)\n",
    "print(\"\\n🧮 Confusion Matrix:\")\n",
    "print(conf_mat)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m28/28\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 255ms/step\n",
      "\n",
      "📊 Per-Class F1-scores:\n",
      "  0: 0.4384\n",
      "  1: 0.9742\n",
      "  2: 0.4925\n",
      "  3: 0.8000\n",
      "\n",
      "📃 Full classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.52      0.44        31\n",
      "           1       0.99      0.96      0.97       746\n",
      "           2       0.48      0.51      0.49        65\n",
      "           3       0.74      0.87      0.80        30\n",
      "\n",
      "    accuracy                           0.91       872\n",
      "   macro avg       0.65      0.71      0.68       872\n",
      "weighted avg       0.92      0.91      0.91       872\n",
      "\n",
      "\n",
      "🧮 Confusion Matrix:\n",
      "[[ 16   0  11   4]\n",
      " [  2 717  25   2]\n",
      " [ 23   6  33   3]\n",
      " [  1   3   0  26]]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 30 times Exp"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T19:24:23.977479Z",
     "start_time": "2025-04-06T03:02:35.643547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMG_SIZE = 128\n",
    "NUM_CLASSES = 4\n",
    "NUM_RUNS = 30\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "\n",
    "metrics = {\n",
    "    'classes': {cls: {'precision': [], 'recall': [], 'f1': []} for cls in class_names},\n",
    "    'macro_avg': {'precision': [], 'recall': [], 'f1': []},\n",
    "    'weighted_avg': {'precision': [], 'recall': [], 'f1': []},\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "def create_resnet_model():\n",
    "    ResNet50_model = ResNet50(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=(128, 128, 3)\n",
    "    )\n",
    "    \n",
    "    # for layer in ResNet50_model.layers[:-10]:\n",
    "    #     layer.trainable = False\n",
    "    # \n",
    "    # for layer in ResNet50_model.layers[-10:]:\n",
    "    #     layer.trainable = True\n",
    "    \n",
    "    \n",
    "    # defined output layer\n",
    "    x = layers.GlobalAveragePooling2D()(ResNet50_model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=ResNet50_model.input, outputs=x)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate=0.001), \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[multi_class_f1]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "TEMP_MODEL_PATH = 'temp_best_model_resnet.keras'\n",
    "\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"\\n=========== Training Run {run + 1}/{NUM_RUNS} ===========\")\n",
    "    \n",
    "    if os.path.exists(TEMP_MODEL_PATH):\n",
    "        os.remove(TEMP_MODEL_PATH)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(run)\n",
    "    tf.random.set_seed(run)\n",
    "\n",
    "    model = create_resnet_model()\n",
    "\n",
    "    checkpoint = ModelCheckpoint('temp_best_model_resnet.keras', monitor='val_multi_class_f1', save_best_only=True, mode='max', verbose=0)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_multi_class_f1', factor=0.5, patience=5, mode='max',verbose=0)\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=30,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[checkpoint, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    best_model = load_model('temp_best_model_resnet.keras', custom_objects={'multi_class_f1': multi_class_f1})\n",
    "\n",
    "    y_pred = best_model.predict(test_generator, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "\n",
    "    report = classification_report(y_true, y_pred_classes,\n",
    "                                   target_names=class_names,\n",
    "                                   output_dict=True,\n",
    "                                   zero_division=1)\n",
    "\n",
    "    for cls in class_names:\n",
    "        metrics['classes'][cls]['precision'].append(report[cls]['precision'])\n",
    "        metrics['classes'][cls]['recall'].append(report[cls]['recall'])\n",
    "        metrics['classes'][cls]['f1'].append(report[cls]['f1-score'])\n",
    "\n",
    "    metrics['macro_avg']['precision'].append(report['macro avg']['precision'])\n",
    "    metrics['macro_avg']['recall'].append(report['macro avg']['recall'])\n",
    "    metrics['macro_avg']['f1'].append(report['macro avg']['f1-score'])\n",
    "\n",
    "    metrics['weighted_avg']['precision'].append(report['weighted avg']['precision'])\n",
    "    metrics['weighted_avg']['recall'].append(report['weighted avg']['recall'])\n",
    "    metrics['weighted_avg']['f1'].append(report['weighted avg']['f1-score'])\n",
    "\n",
    "    metrics['accuracy'].append(report['accuracy'])\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Training Run 1/30 ===========\n",
      "WARNING:tensorflow:From C:\\Users\\10302\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10302\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "C:\\Users\\10302\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "C:\\Users\\10302\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Training Run 2/30 ===========\n",
      "\n",
      "=========== Training Run 3/30 ===========\n",
      "\n",
      "=========== Training Run 4/30 ===========\n",
      "\n",
      "=========== Training Run 5/30 ===========\n",
      "\n",
      "=========== Training Run 6/30 ===========\n",
      "\n",
      "=========== Training Run 7/30 ===========\n",
      "\n",
      "=========== Training Run 8/30 ===========\n",
      "\n",
      "=========== Training Run 9/30 ===========\n",
      "\n",
      "=========== Training Run 10/30 ===========\n",
      "\n",
      "=========== Training Run 11/30 ===========\n",
      "\n",
      "=========== Training Run 12/30 ===========\n",
      "\n",
      "=========== Training Run 13/30 ===========\n",
      "\n",
      "=========== Training Run 14/30 ===========\n",
      "\n",
      "=========== Training Run 15/30 ===========\n",
      "\n",
      "=========== Training Run 16/30 ===========\n",
      "\n",
      "=========== Training Run 17/30 ===========\n",
      "\n",
      "=========== Training Run 18/30 ===========\n",
      "\n",
      "=========== Training Run 19/30 ===========\n",
      "\n",
      "=========== Training Run 20/30 ===========\n",
      "\n",
      "=========== Training Run 21/30 ===========\n",
      "\n",
      "=========== Training Run 22/30 ===========\n",
      "\n",
      "=========== Training Run 23/30 ===========\n",
      "\n",
      "=========== Training Run 24/30 ===========\n",
      "\n",
      "=========== Training Run 25/30 ===========\n",
      "\n",
      "=========== Training Run 26/30 ===========\n",
      "\n",
      "=========== Training Run 27/30 ===========\n",
      "\n",
      "=========== Training Run 28/30 ===========\n",
      "\n",
      "=========== Training Run 29/30 ===========\n",
      "\n",
      "=========== Training Run 30/30 ===========\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T19:24:24.095701Z",
     "start_time": "2025-04-06T19:24:24.074987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\\n=== Classification Report with Mean±Std ===\")\n",
    "\n",
    "class_col_width = max(len(str(cls)) for cls in class_names) + 2\n",
    "metric_width = 14\n",
    "support_width = 8\n",
    "\n",
    "print(f\"\\n{'Class':<{class_col_width}} {'Precision':<{metric_width}} {'Recall':<{metric_width}} {'F1-score':<{metric_width}} {'Support':<{support_width}}\")\n",
    "\n",
    "for idx, cls in enumerate(class_names):\n",
    "    prec_mean = np.mean(metrics['classes'][cls]['precision'])\n",
    "    prec_std = np.std(metrics['classes'][cls]['precision'])\n",
    "    rec_mean = np.mean(metrics['classes'][cls]['recall'])\n",
    "    rec_std = np.std(metrics['classes'][cls]['recall'])\n",
    "    f1_mean = np.mean(metrics['classes'][cls]['f1'])\n",
    "    f1_std = np.std(metrics['classes'][cls]['f1'])\n",
    "    support = test_generator.classes.tolist().count(idx)\n",
    "\n",
    "    print(f\"{cls:<{class_col_width}} \"\n",
    "          f\"{prec_mean:.2f}±{prec_std:.2f}  \"\n",
    "          f\"{rec_mean:.2f}±{rec_std:.2f}  \"\n",
    "          f\"{f1_mean:.2f}±{f1_std:.2f}  \"\n",
    "          f\"{support:<{support_width}}\")\n",
    "\n",
    "def print_avg_row(name, metric_dict):\n",
    "    prec = f\"{np.mean(metric_dict['precision']):.2f}±{np.std(metric_dict['precision']):.2f}\"\n",
    "    rec = f\"{np.mean(metric_dict['recall']):.2f}±{np.std(metric_dict['recall']):.2f}\"\n",
    "    f1 = f\"{np.mean(metric_dict['f1']):.2f}±{np.std(metric_dict['f1']):.2f}\"\n",
    "    print(f\"{name:<{class_col_width}} {prec:<{metric_width}} {rec:<{metric_width}} {f1:<{metric_width}}\")\n",
    "\n",
    "# accuracy\n",
    "acc_mean = np.mean(metrics['accuracy'])\n",
    "acc_std = np.std(metrics['accuracy'])\n",
    "total_samples = len(test_generator.classes)\n",
    "\n",
    "print(f\"\\n{'accuracy':<{class_col_width}} {'':<{metric_width}} {'':<{metric_width}} \"\n",
    "      f\"{acc_mean:.2f}±{acc_std:.2f}  {total_samples:<{support_width}}\")\n",
    "\n",
    "print_avg_row('macro avg', metrics['macro_avg'])\n",
    "print_avg_row('weighted avg', metrics['weighted_avg'])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Classification Report with Mean±Std ===\n",
      "\n",
      "Class Precision      Recall         F1-score       Support \n",
      "0   0.38±0.17  0.26±0.20  0.27±0.16  31      \n",
      "1   0.97±0.02  0.96±0.05  0.96±0.03  746     \n",
      "2   0.52±0.12  0.62±0.18  0.54±0.12  65      \n",
      "3   0.71±0.17  0.65±0.18  0.65±0.14  30      \n",
      "\n",
      "accuracy                               0.90±0.05  872     \n",
      "macro avg 0.65±0.09      0.62±0.07      0.61±0.07     \n",
      "weighted avg 0.91±0.02      0.90±0.05      0.90±0.03     \n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
